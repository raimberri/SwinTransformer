{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from timm.models.layers import trunc_normal_, DropPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5)#.contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Shifted) window self-attention module\n",
    "class WMSA(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, head_dim, window_size, module_type):\n",
    "        super(WMSA, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.n_heads = input_dim // head_dim\n",
    "        self.window_size = window_size\n",
    "        self.module_type = module_type\n",
    "        self.embedding_layer = nn.Linear(self.input_dim, 3*self.input_dim, bias = True)\n",
    "        self.relative_position_params = nn.Parameter(torch.zeros((2*window_size - 1)*(2*window_size - 1), self.n_heads))\n",
    "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
    "        \n",
    "        trunc_normal_(self.relative_position_params, std=0.02)\n",
    "        self.relative_position_params = torch.nn.Parameter(self.relative_position_params.view(2*window_size - 1, 2*window_size - 1, self.n_heads).transpose(1,2).transpose(0,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, H, W, _ = x.shape\n",
    "        #pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "        #do cyclic shift if it is SWMSA\n",
    "        if self.module_type != 'W': \n",
    "            x = torch.roll(x, shifts=(-(self.window_size//2), -(self.window_size//2)), dims = (1,2))\n",
    "        #window partition in one line\n",
    "        #x = window_partition(x, self.window_size)\n",
    "        x = rearrange(x, 'b (w1 p1) (w2 p2) c -> b w1 w2 p1 p2 c', p1=self.window_size, p2=self.window_size)\n",
    "        h_window_num = x.size(1)\n",
    "        w_window_num = x.size(2)\n",
    "        \n",
    "        #window num validation\n",
    "        #assert h_window_num == w_window_num\n",
    "        \n",
    "        x = rearrange(x, 'b w1 w2 p1 p2 c -> b (w1 w2) (p1 p2) c', p1=self.window_size, p2=self.window_size)\n",
    "        qkv = self.embedding_layer(x)\n",
    "        q, k, v = rearrange(qkv, 'b nw np (three c) -> three b nw np c', c=self.head_dim).chunk(3, dim=0)\n",
    "        #compute q*k.T using einsum\n",
    "        attn = torch.einsum('hbwpc, hbwqc->hbwpq', q, k) * self.scale\n",
    "        #add relative embedding\n",
    "        attn = attn + rearrange(self.relative_embedding(), 'h p q -> h 1 1 p q')\n",
    "        #if this module is SWSA\n",
    "        if self.module_type != 'W':\n",
    "            attn_mask = self.generate_mask(h_window_num, w_window_num, self.window_size, shift=self.window_size//2)\n",
    "            attn = attn.masked_fill_(attn_mask, float(\"-inf\"))\n",
    "        \n",
    "        attn = nn.functional.softmax(attn, dim=-1)\n",
    "        output = torch.einsum('hbwij,hbwjc->hbwic', attn, v)\n",
    "        output = rearrange(output, 'h b w p c -> b w p (h c)')\n",
    "        output = self.linear(output)\n",
    "        output = rearrange(output, 'b (w1 w2) (p1 p2) c -> b (w1 p1) (w2 p2) c', w1 = h_window_num, p1 = self.window_size)\n",
    "        \n",
    "        #undo cyclic shift if it is SWMSA\n",
    "        if self.type!='W': \n",
    "            output = torch.roll(output, shifts=(self.window_size//2, self.window_size//2), dims=(1,2))\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            output = output[:, :H, :W, :].contiguous()\n",
    "        return output\n",
    "    \n",
    "    def generate_mask(self, w1, w2, p, shift):\n",
    "        \"\"\"generate SWMSA mask\"\"\"\n",
    "        attn_mask = torch.zeros(w1, w2, p, p, p, p, dtype=torch.bool, device=self.relative_position_params.device)\n",
    "        if self.module_type == 'W':\n",
    "            return attn_mask\n",
    "        \n",
    "        s = p - shift\n",
    "        #since the attn masks are already given in the offical implementation, we directly construct it here\n",
    "        #construct attn masks of the last row windows(window2) in two lines\n",
    "        attn_mask[-1, :, :s, :, s:, :] = True\n",
    "        attn_mask[-1, :, s:, :, :s, :] = True\n",
    "        #construct attn masks of last colomn windows(window1) in two lines\n",
    "        attn_mask[:, -1, :, :s, :, s:] = True\n",
    "        attn_mask[:, -1, :, s:, :, :s] = True\n",
    "        #window3 is automatically constructed by operations above and window0 is already filled by zeros\n",
    "        #reshape the attention mask to (1 1 nW p^2 p^2)\n",
    "        attn_mask = rearrange(attn_mask, 'w1 w2 p1 p2 p3 p4 -> 1 1 (w1 w2) (p1 p2) (p3 p4)')\n",
    "        return attn_mask\n",
    "    \n",
    "    def relative_embedding(self):\n",
    "        cord = torch.tensor(np.array([[i,j] for i in range(self.window_size) for j in range(self.window_size)]))\n",
    "        #use broadcast to calculate relative relation\n",
    "        relation = cord[:, None, :] - cord[None, : ,:] + self.window_size - 1\n",
    "        return self.relative_position_params[:, relation[:,:,0], relation[:,:,1]]\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, head_dim, window_size, drop_path=0, module_type='W', img_size=None):\n",
    "        \"\"\"Swin transformer block\"\"\"\n",
    "        super(Block, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        assert module_type in ['W', 'SW']\n",
    "        self.module_type = module_type\n",
    "        if img_size <= window_size:\n",
    "            self.module_type = 'W'\n",
    "            \n",
    "        print(\"Block Initial Type: {}, drop_path_rate:{:.6f}\".format(self.module_type, drop_path))\n",
    "        self.ln1 = nn.LayerNorm(input_dim)\n",
    "        self.msa = WMSA(input_dim, input_dim, head_dim, window_size, self.module_type)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
    "        self.ln2 = nn.LayerNorm(input_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4*input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*input_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.msa(self.ln1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.ln2(x)))\n",
    "        return x\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, depths_config=[2,2,6,2], embed_dim=96, head_dim=32, window_size=7, drop_path_rate=0.2, img_size=224):\n",
    "        super(SwinTransformer, self).__init__()\n",
    "        self.depths_config = depths_config\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths_config))]\n",
    "        \n",
    "        begin = 0\n",
    "        self.stage1 = [nn.Conv2d(3, embed_dim, kernel_size=4, stride=4),\n",
    "                      Rearrange('b c h w -> b h w c'),\n",
    "                      nn.LayerNorm(embed_dim),] + \\\n",
    "                      [Block(embed_dim, embed_dim, head_dim, window_size, dpr[i+begin], 'W' if not i%2 else 'SW', img_size//4)\n",
    "                      for i in range(depths_config[0])]\n",
    "        begin += depths_config[0]\n",
    "        self.stage2 = [Rearrange('b (h neih) (w neiw) c -> b h w (neiw neih c)', neih=2, neiw=2),\n",
    "                      nn.LayerNorm(4*embed_dim), nn.Linear(4*embed_dim, 2*embed_dim, bias=False),] + \\\n",
    "                      [Block(2*embed_dim, 2*embed_dim, head_dim, window_size, dpr[i+begin], 'W' if not i%2 else 'SW', img_size//8)\n",
    "                      for i in range(depths_config[1])]\n",
    "        begin += depths_config[1]\n",
    "        self.stage3 = [Rearrange('b (h neih) (w neiw) c -> b h w (neiw neih c)', neih=2, neiw=2),\n",
    "                      nn.LayerNorm(8*embed_dim), nn.Linear(8*embed_dim, 4*embed_dim, bias=False),] + \\\n",
    "                      [Block(4*embed_dim, 4*embed_dim, head_dim, window_size, dpr[i+begin], 'W' if not i%2 else 'SW', img_size//16)\n",
    "                      for i in range(depths_config[2])]\n",
    "        begin += depths_config[2]\n",
    "        self.stage4 = [Rearrange('b (h neih) (w neiw) c -> b h w (neiw neih c)', neih=2, neiw=2),\n",
    "                      nn.LayerNorm(16*embed_dim), nn.Linear(16*embed_dim, 8*embed_dim, bias=False),] + \\\n",
    "                      [Block(8*embed_dim, 8*embed_dim, head_dim, window_size, dpr[i+begin], 'W' if not i%2 else 'SW', img_size//16)\n",
    "                      for i in range(depths_config[3])]\n",
    "        \n",
    "        self.stage1 = nn.Sequential(*self.stage1)\n",
    "        self.stage2 = nn.Sequential(*self.stage2)\n",
    "        self.stage3 = nn.Sequential(*self.stage3)\n",
    "        self.stage4 = nn.Sequential(*self.stage4)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(8*embed_dim)\n",
    "        self.avgpool = Reduce('b h w c -> b c', reduction='mean')\n",
    "        self.head = nn.Linear(8*embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        #padding before patch embedding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % 4 != 0:\n",
    "            x = F.pad(x, (0, 4 - W % 4))\n",
    "        if H % 4 != 0:\n",
    "            x = F.pad(x, (0, 0, 0, 4 - H % 4))\n",
    "        x = self.stage1(x)\n",
    "        print(x.size())\n",
    "        #padding before patch merging\n",
    "        _, H, W, _ = x.size()\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "        x = self.stage2(x)\n",
    "        print(x.size())\n",
    "        #padding before patch merging\n",
    "        _, H, W, _ = x.size()\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "        x = self.stage3(x)\n",
    "        print(x.size())\n",
    "        #padding before patch merging\n",
    "        _, H, W, _ = x.size()\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "        x = self.stage4(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Swin_T(num_classes, depths_config=[2,2,6,2], embed_dim=96, **kwargs):\n",
    "    return SwinTransformer(num_classes, depths_config=depths_config, embed_dim=embed_dim, **kwargs)\n",
    "\n",
    "def Swin_S(num_classes, depths_config=[2,2,18,2], embed_dim=96, **kwargs):\n",
    "    return SwinTransformer(num_classes, depths_config=depths_config, embed_dim=embed_dim, **kwargs)\n",
    "\n",
    "def Swin_B(num_classes, depths_config=[2,2,18,2], embed_dim=128, **kwargs):\n",
    "    return SwinTransformer(num_classes, depths_config=depths_config, embed_dim=embed_dim, **kwargs)\n",
    "\n",
    "def Swin_L(num_classes, depths_config=[2,2,18,2], embed_dim=192, **kwargs):\n",
    "    return SwinTransformer(num_classes, depths_config=depths_config, embed_dim=embed_dim, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Initial Type: W, drop_path_rate:0.000000\n",
      "Block Initial Type: SW, drop_path_rate:0.018182\n",
      "Block Initial Type: W, drop_path_rate:0.036364\n",
      "Block Initial Type: SW, drop_path_rate:0.054545\n",
      "Block Initial Type: W, drop_path_rate:0.072727\n",
      "Block Initial Type: SW, drop_path_rate:0.090909\n",
      "Block Initial Type: W, drop_path_rate:0.109091\n",
      "Block Initial Type: SW, drop_path_rate:0.127273\n",
      "Block Initial Type: W, drop_path_rate:0.145455\n",
      "Block Initial Type: SW, drop_path_rate:0.163636\n",
      "Block Initial Type: W, drop_path_rate:0.181818\n",
      "Block Initial Type: SW, drop_path_rate:0.200000\n",
      "torch.Size([3, 3, 736, 972])\n",
      "torch.Size([3, 184, 243, 96])\n",
      "torch.Size([3, 92, 122, 192])\n",
      "torch.Size([3, 46, 61, 384])\n",
      "28288354\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_model = Swin_T(1000).cuda()\n",
    "    n_parameters = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "    #print(test_model)\n",
    "    dummy_input = torch.rand(3,3,736,972).cuda()\n",
    "    output = test_model(dummy_input)\n",
    "    #print(output.size())\n",
    "    print(n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Initial Type: W, drop_path_rate:0.000000\n",
      "Block Initial Type: SW, drop_path_rate:0.018182\n",
      "Block Initial Type: W, drop_path_rate:0.036364\n",
      "Block Initial Type: SW, drop_path_rate:0.054545\n",
      "Block Initial Type: W, drop_path_rate:0.072727\n",
      "Block Initial Type: SW, drop_path_rate:0.090909\n",
      "Block Initial Type: W, drop_path_rate:0.109091\n",
      "Block Initial Type: SW, drop_path_rate:0.127273\n",
      "Block Initial Type: W, drop_path_rate:0.145455\n",
      "Block Initial Type: SW, drop_path_rate:0.163636\n",
      "Block Initial Type: W, drop_path_rate:0.181818\n",
      "Block Initial Type: SW, drop_path_rate:0.200000\n"
     ]
    }
   ],
   "source": [
    "backbone = nn.Sequential(\n",
    "            *list(Swin_T(num_classes=91, drop_path_rate=0.2).children())\n",
    "        )[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbackbone\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/nn/modules/container.py:107\u001b[0m, in \u001b[0;36mSequential.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(OrderedDict(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems())[idx]))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_by_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/nn/modules/container.py:98\u001b[0m, in \u001b[0;36mSequential._get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m     96\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m-\u001b[39msize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m size:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx))\n\u001b[1;32m     99\u001b[0m idx \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(islice(iterator, idx, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of range"
     ]
    }
   ],
   "source": [
    "print(backbone[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
